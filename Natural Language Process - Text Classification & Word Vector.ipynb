{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part1: https://www.kaggle.com/matleonard/intro-to-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy is the leading library for NLP, and it has quickly become one of the most popular Python frameworks. Most people find it intuitive, and it has excellent documentation.\n",
    "# spaCy relies on models that are language-specific and come in different sizes. You can load a spaCy model with spacy.load.\n",
    "# For example, here's how you would load the English language model.\n",
    "\n",
    "nlp=spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tea is healthy and calming, don't you think?\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(\"Tea is healthy and calming, don't you think?\")\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tea\n",
      "is\n",
      "healthy\n"
     ]
    }
   ],
   "source": [
    "# Tokenization： A token is a unit of text in the document, such as individual words and punctuation. SpaCy splits contractions like \"don't\" into two tokens, \"do\" and \"n't\". You can see the tokens by iterating through the document.\n",
    "for token in doc[0:3]:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "1. lemmatizing: lemma is the base form of a word（lemmatization）\n",
    "2. Remove stopwords: stopwords are words that occur frequently in the language and don't contain much information. English stopwords include \"the\", \"is\", \"and\", \"but\", \"not\". Remove stopword help predictive models focus on relevant words.\n",
    "3. stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token\t\tLemma\t\tStopword\n",
      "--------------------------------------------------\n",
      "Tea\t\ttea\t\tFalse\n",
      "is\t\tbe\t\tTrue\n",
      "healthy\t\thealthy\t\tFalse\n",
      "and\t\tand\t\tTrue\n",
      "calming\t\tcalm\t\tFalse\n",
      ",\t\t,\t\tFalse\n",
      "do\t\tdo\t\tTrue\n",
      "n't\t\tnot\t\tTrue\n",
      "you\t\t-PRON-\t\tTrue\n",
      "think\t\tthink\t\tFalse\n",
      "?\t\t?\t\tFalse\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token\\t\\tLemma\\t\\tStopword\".format('Token','Lemma','Stopword'))\n",
    "print(\"-\"*50)\n",
    "for Token in doc:\n",
    "    print(f\"{Token}\\t\\t{Token.lemma_}\\t\\t{Token.is_stop}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern Matching¶\n",
    "To match individual tokens, you create a Matcher. When you want to match a list of terms, it's easier and more efficient to use PhraseMatcher. \n",
    "\n",
    "For example, if you want to find where different smartphone models show up in some text, you can create patterns for the model names of interest. First you create the PhraseMatcher itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab, attr='LOWER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you create a list of terms to match in the text. The phrase matcher needs the patterns as document objects. The easiest way to get these is with a list comprehension using the nlp model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3766102292120407359, 17, 19), (3766102292120407359, 22, 24), (3766102292120407359, 30, 32), (3766102292120407359, 33, 35)]\n"
     ]
    }
   ],
   "source": [
    "terms = ['Galaxy Note', 'iPhone 11', 'iPhone XS', 'Google Pixel']\n",
    "patterns = [nlp(text) for text in terms]\n",
    "matcher.add(\"TerminologyList\", None, *patterns)\n",
    "# Borrowed from https://daringfireball.net/linked/2019/09/21/patel-11-pro\n",
    "text_doc = nlp(\"Glowing review overall, and some really interesting side-by-side \"\n",
    "               \"photography tests pitting the iPhone 11 Pro against the \"\n",
    "               \"Galaxy Note 10 Plus and last year’s iPhone XS and Google Pixel 3.\") \n",
    "matches = matcher(text_doc)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matches here are a tuple of the match id and the positions of the start and end of the phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TerminologyList iPhone XS\n"
     ]
    }
   ],
   "source": [
    "match_id, start, end = matches[2]\n",
    "print(nlp.vocab.strings[match_id],text_doc[start:end])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: https://www.kaggle.com/matleonard/text-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Classification: This is \"classification\" in the conventional machine learning sense, and it is applied to text. Examples include spam detection, sentiment analysis, and tagging customer queries.\n",
    "spam = pd.read_csv(\"/Users/shiwenwang/Machine Learning Project/Data/spam.csv\",encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = spam[['v1','v2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.columns=['label','text']\n",
    "spam.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning models don't learn from raw text data. Instead, you need to convert the text to something numeric.\n",
    "\n",
    "The simplest common representation is a variation of one-hot encoding. You represent each document as a vector of term frequencies for each term in the vocabulary. The vocabulary is built from all the tokens (terms) in the corpus (the collection of documents).\n",
    "\n",
    "Another common representation is TF-IDF (Term Frequency - Inverse Document Frequency). TF-IDF is similar to bag of words except that each term count is scaled by the term's frequency in the corpus. Using TF-IDF can potentially improve your models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TextCategorizer is a spaCy pipe. Pipes are classes for processing and transforming tokens. When you create a spaCy model with nlp = spacy.load('en_core_web_sm'), there are default pipes that perform part of speech tagging, entity recognition, and other transformations. When you run text through a model doc = nlp(\"Some text here\"), the output of the pipes are attached to the tokens in the doc object. The lemmas for token.lemma_ come from one of these pipes.\n",
    "\n",
    "You can remove or add pipes to models. What we'll do here is create an empty model without any pipes (other than a tokenizer, since all models always have a tokenizer). Then, we'll create a TextCategorizer pipe and add it to the empty model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Create an empty model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create the TextCategorizer with exclusive classes and \"bow\" architecture\n",
    "textcat = nlp.create_pipe(\n",
    "              \"textcat\",\n",
    "              config={\n",
    "                \"exclusive_classes\": True,\n",
    "                \"architecture\": \"bow\"})\n",
    "\n",
    "# Add the TextCategorizer to the empty model\n",
    "nlp.add_pipe(textcat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the classes are either ham or spam, we set \"exclusive_classes\" to True. We've also configured it with the bag of words (\"bow\") architecture. spaCy provides a convolutional neural network architecture as well, but it's more complex than you need for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add labels to text classifier\n",
    "textcat.add_label(\"ham\")\n",
    "textcat.add_label(\"spam\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Text Categorizer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you'll convert the labels in the data to meet the TextCategorizer requires. For each document, you'll create a dictionary of boolean values for each class.For example, if a text is \"ham\", we need a dictionary {'ham': True, 'spam': False}. The model is looking for these labels inside another dictionary with the key 'cats'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cats': {'ham': True, 'spam': False}},\n",
       " {'cats': {'ham': True, 'spam': False}},\n",
       " {'cats': {'ham': False, 'spam': True}},\n",
       " {'cats': {'ham': True, 'spam': False}},\n",
       " {'cats': {'ham': True, 'spam': False}},\n",
       " {'cats': {'ham': False, 'spam': True}},\n",
       " {'cats': {'ham': True, 'spam': False}},\n",
       " {'cats': {'ham': True, 'spam': False}},\n",
       " {'cats': {'ham': False, 'spam': True}},\n",
       " {'cats': {'ham': False, 'spam': True}}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts=spam['text'].values\n",
    "train_labels = [{'cats':{'ham':label=='ham',\n",
    "                        'spam':label=='spam'}}\n",
    "               for label in spam['label']]\n",
    "\n",
    "train_labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
       "  {'cats': {'ham': True, 'spam': False}}),\n",
       " ('Ok lar... Joking wif u oni...', {'cats': {'ham': True, 'spam': False}}),\n",
       " (\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
       "  {'cats': {'ham': False, 'spam': True}})]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#next combine texts and labels into a single list\n",
    "train_data = list(zip(train_texts,train_labels))\n",
    "train_data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model\n",
    "First, create an optimizer using nlp.begin_training(). spaCy uses this optimizer to update the model. In general it's more efficient to train models in small batches. spaCy provides the minibatch function that returns a generator yielding minibatches for training. Finally, the minibatches are split into texts and labels, then used with nlp.update to update the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import minibatch\n",
    "\n",
    "spacy.util.fix_random_seed(1)\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "# Create the batch generator with batch size = 8\n",
    "batches = minibatch(train_data, size=8)\n",
    "# Iterate through minibatches\n",
    "for batch in batches:\n",
    "    # Each batch is a list of (text, label) but we need to\n",
    "    # send separate lists for texts and labels to update().\n",
    "    # This is a quick way to split a list of tuples into lists\n",
    "    texts, labels = zip(*batch)\n",
    "    nlp.update(texts, labels, sgd=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'textcat': 0.43193236928277656}\n",
      "{'textcat': 0.647564146202626}\n",
      "{'textcat': 0.7843300964563298}\n",
      "{'textcat': 0.8716628925737462}\n",
      "{'textcat': 0.9279837872985808}\n",
      "{'textcat': 0.9654547567888858}\n",
      "{'textcat': 0.993859635343282}\n",
      "{'textcat': 1.0126842369795321}\n",
      "{'textcat': 1.0274316810190278}\n",
      "{'textcat': 1.0376987757931415}\n"
     ]
    }
   ],
   "source": [
    "# above is just one epoch; the model need multiple epochs, use another loop for more epochs \n",
    "# and optional re-shuffle the training data at the begining of each loop\n",
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "spacy.util.fix_random_seed(1)\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "losses = {}\n",
    "for epoch in range(10):\n",
    "    random.shuffle(train_data)\n",
    "    # Create the batch generator with batch size = 8\n",
    "    batches = minibatch(train_data, size=8)\n",
    "    # Iterate through minibatches\n",
    "    for batch in batches:\n",
    "        # Each batch is a list of (text, label) but we need to\n",
    "        # send separate lists for texts and labels to update().\n",
    "        # This is a quick way to split a list of tuples into lists\n",
    "        texts, labels = zip(*batch)\n",
    "        nlp.update(texts, labels, sgd=optimizer, losses=losses)\n",
    "    print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a trained model, you can make predictions with the predict() method. The input text needs to be tokenized with nlp.tokenizer. Then you pass the tokens to the predict method which returns scores. The scores are the probability the input text belongs to the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"Are you ready for the tea party????? It's gonna be wild\",\n",
    "         \"URGENT Reply to this message for GUARANTEED FREE TEA\",\n",
    "        \"Buy one get one free tea!!\",\n",
    "        \"VOTE for TEA!\"]\n",
    "docs = [nlp.tokenizer(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "textcat = nlp.get_pipe('textcat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores,_ = textcat.predict(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9994242e-01 5.7520185e-05]\n",
      " [1.1534694e-02 9.8846531e-01]\n",
      " [9.5188344e-01 4.8116509e-02]\n",
      " [8.2792711e-01 1.7207293e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'spam', 'ham', 'ham']\n"
     ]
    }
   ],
   "source": [
    "# From the scores, find the label with the highest score/probability\n",
    "predicted_labels = scores.argmax(axis=1)\n",
    "print([textcat.labels[label] for label in predicted_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Word Embeddings https://www.kaggle.com/matleonard/word-vectors\n",
    "\n",
    "Word Embedding is just to convert words into vectors. \n",
    "\n",
    "These vectors can be used as features for machine learning models. Word vectors will typically improve the performance of your models above bag of words encoding. spaCy provides embeddings learned from a model called Word2Vec. You can access them by loading a large language model like en_core_web_lg. Then they will be available on tokens from the .vector attribute.\n",
    "\n",
    "word2vec is a neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_lg\n",
    "import numpy as np\n",
    "\n",
    "# Need to load the large model to get the vectors\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Disabling other pipes because we don't need them and it'll speed up this part a bit\n",
    "text = \"These vectors can be used as features for machine learning models.\"\n",
    "with nlp.disable_pipes():\n",
    "    vectors = np.array([token.vector for token in  nlp(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 300)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are 300-dimensional vectors, with one vector for each word. However, we only have document-level labels and our models won't be able to use the word-level embeddings. So, you need a vector representation for the entire document.\n",
    "\n",
    "There are many ways to combine all the word vectors into a single document vector we can use for model training. A simple and surprisingly effective approach is simply averaging the vectors for each word in the document. Then, you can use these document vectors for modeling.\n",
    "\n",
    "spaCy calculates the average document vector which you can get with doc.vector. Here is an example loading the spam data and converting it to document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 300)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with nlp.disable_pipes():\n",
    "    doc_vectors = np.array([nlp(text).vector for text in spam.text])\n",
    "    \n",
    "doc_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Models\n",
    "With the document vectors, you can train scikit-learn models, xgboost models, or any other standard approach to modeling.\n",
    "\n",
    "Here, we use svm classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.312%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(doc_vectors, spam.label,\n",
    "                                                    test_size=0.1, random_state=1)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Set dual=False to speed up training, and it's not needed\n",
    "svc = LinearSVC(random_state=1, dual=False, max_iter=10000)\n",
    "svc.fit(X_train, y_train)\n",
    "print(f\"Accuracy: {svc.score(X_test, y_test) * 100:.3f}%\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Similarity\n",
    "Documents with similar content generally have similar vectors. So you can find similar documents by measuring the similarity between the vectors. A common metric for this is the cosine similarity which measures the angle between two vectors,  𝐚  and  𝐛.\n",
    "\n",
    "cos𝜃=𝐚⋅𝐛/‖𝐚‖‖𝐛‖\n",
    " \n",
    "This is the dot product of  𝐚  and  𝐛 , divided by the magnitudes of each vector. The cosine similarity can vary between -1 and 1, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7030031"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return a.dot(b)/np.sqrt(a.dot(a) * b.dot(b))\n",
    "\n",
    "a = nlp(\"REPLY NOW FOR FREE TEA\").vector\n",
    "b = nlp(\"According to legend, Emperor Shen Nung discovered tea when leaves from a wild tree blew into his pot of boiling water.\").vector\n",
    "cosine_similarity(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
